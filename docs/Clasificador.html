<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>4&nbsp; Clasificador de Neumonias con Deep Learning – Módulos 5 y 6</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./Entrenamiento.html" rel="next">
<link href="./colaboracion.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>
<script src="https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js" crossorigin="anonymous"></script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./Clasificador.html"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Clasificador de Neumonias con Deep Learning</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Módulos 5 y 6</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Temario y referencias</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introducción</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./plan-estrategico.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Planteamiento Estratégico</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./colaboracion.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Colaboración Interdisciplinaria en Proyectos de IA</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Clasificador.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Clasificador de Neumonias con Deep Learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Entrenamiento.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Entrenamiento</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Interpretacion.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Visualizar la decisión del clasificador</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Evaluacion.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Evaluación e Implementación</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./plan-neumonia.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Plan de Proyecto para la Clasificación de Neumonías con Inteligencia Artificial</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./presentacion.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Presentación del Proyecto (<em>Pitch</em>)</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introducción" id="toc-introducción" class="nav-link active" data-scroll-target="#introducción"><span class="header-section-number">4.1</span> Introducción</a></li>
  <li><a href="#vision-por-computadora" id="toc-vision-por-computadora" class="nav-link" data-scroll-target="#vision-por-computadora"><span class="header-section-number">4.2</span> Vision por Computadora</a>
  <ul class="collapse">
  <li><a href="#filtros" id="toc-filtros" class="nav-link" data-scroll-target="#filtros"><span class="header-section-number">4.2.1</span> Filtros</a></li>
  </ul></li>
  <li><a href="#cnns" id="toc-cnns" class="nav-link" data-scroll-target="#cnns"><span class="header-section-number">4.3</span> CNNs</a>
  <ul class="collapse">
  <li><a href="#pooling" id="toc-pooling" class="nav-link" data-scroll-target="#pooling"><span class="header-section-number">4.3.1</span> Pooling</a></li>
  </ul></li>
  <li><a href="#obtención-de-datos" id="toc-obtención-de-datos" class="nav-link" data-scroll-target="#obtención-de-datos"><span class="header-section-number">4.4</span> Obtención de Datos</a>
  <ul class="collapse">
  <li><a href="#consideraciones" id="toc-consideraciones" class="nav-link" data-scroll-target="#consideraciones"><span class="header-section-number">4.4.1</span> <strong>Consideraciones</strong></a></li>
  </ul></li>
  <li><a href="#preprocesamiento" id="toc-preprocesamiento" class="nav-link" data-scroll-target="#preprocesamiento"><span class="header-section-number">4.5</span> Preprocesamiento</a>
  <ul class="collapse">
  <li><a href="#exploración" id="toc-exploración" class="nav-link" data-scroll-target="#exploración"><span class="header-section-number">4.5.1</span> Exploración</a></li>
  <li><a href="#preprocesamiento-de-imagenes" id="toc-preprocesamiento-de-imagenes" class="nav-link" data-scroll-target="#preprocesamiento-de-imagenes"><span class="header-section-number">4.5.2</span> Preprocesamiento de Imagenes</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Clasificador de Neumonias con Deep Learning</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="introducción" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="introducción"><span class="header-section-number">4.1</span> Introducción</h2>
<div class="panel">
<div style="position: relative; width: 100%; height: 0; padding-top: 250.0000%; padding-bottom: 0; box-shadow: 0 2px 8px 0 rgba(63,69,81,0.16); margin-top: 1.6em; margin-bottom: 0.9em; overflow: hidden; border-radius: 8px; will-change: transform;">
<p><iframe loading="lazy" style="position: absolute; width: 100%; height: 100%; top: 0; left: 0; border: none; padding: 0;margin: 0;" src="https://www.canva.com/design/DAGQmio9YL8/dd-yo38tdXanFZMRseWBrA/watch?embed" allowfullscreen="allowfullscreen" allow="fullscreen"> </iframe></p>
</div>
<p><a href="https://www.canva.com/design/DAGQmio9YL8/dd-yo38tdXanFZMRseWBrA/watch?utm_content=DAGQmio9YL8&amp;utm_campaign=designshare&amp;utm_medium=embeds&amp;utm_source=link" target="_blank" rel="noopener">1936</a> de David Escudero</p>
</div>
<p>Gran parte del impulso actual en inteligencia artificial (IA), está estrechamente vinculado al éxito de las redes neuronales profundas (<em>DNN</em>). La era de las <em>DNN</em> no habría emergido sin la confluencia de cuatro factores clave.</p>
<ol type="1">
<li><p>El primero es la disponibilidad de grandes conjuntos de datos, o <em>big data</em>, esenciales para entrenar estos modelos.</p></li>
<li><p>El segundo factor son las unidades de procesamiento gráfico (<em>GPU</em>), originalmente diseñadas para videojuegos, que permiten ejecutar cálculos intensivos mediante arquitecturas paralelas masivas.</p></li>
<li><p>El tercer componente es la computación en la nube, que ofrece almacenamiento y procesamiento de enormes cantidades de datos de manera económica y eficiente.</p></li>
<li><p>Finalmente, el cuarto factor son las bibliotecas de código abierto para el desarrollo de algoritmos, como TensorFlow de Google, Cognitive Toolkit de Microsoft, Caffe de UC Berkeley, PyTorch de Facebook, y Paddle de Baidu.</p></li>
</ol>
<p>Una <em>DNN</em> se puede visualizar como un sándwich de múltiples capas, pero en lugar de ingredientes estáticos, los datos fluyen a través de una serie de cálculos que extraen características de alto nivel a partir de datos sensoriales en bruto. Lo notable es que las capas de estas redes no son diseñadas explícitamente por humanos; más bien, se ajustan mediante técnicas como la retropropagación.</p>
<p>ImageNet ejemplifica un adagio sobre la IA: los conjuntos de datos—no los algoritmos—podrían ser el factor limitante clave para alcanzar una inteligencia artificial a nivel humano.</p>
<blockquote class="blockquote">
<p>“Considero que los datos de píxeles en imágenes y videos son la materia oscura de Internet.” Fej LI</p>
</blockquote>
<p>Muchos diferentes <em>DNNs</em> convolucionales se utilizaron para clasificar las imágenes en los concursos anuales del <em>ImageNet Challenge</em> para reconocer a los mejores (como AlexNet, GoogleNet, VGG Net y ResNet).</p>
<div class="panel">
<div style="position: relative; width: 100%; height: 0; padding-top: 56.25%; overflow: hidden;">
<p><iframe src="https://www.youtube.com/embed/40riCqvRoMs" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border: 0;" allowfullscreen=""></iframe></p>
</div>
</div>
</section>
<section id="vision-por-computadora" class="level2" data-number="4.2">
<h2 data-number="4.2" class="anchored" data-anchor-id="vision-por-computadora"><span class="header-section-number">4.2</span> Vision por Computadora</h2>
<section id="filtros" class="level3" data-number="4.2.1">
<h3 data-number="4.2.1" class="anchored" data-anchor-id="filtros"><span class="header-section-number">4.2.1</span> Filtros</h3>
<p>Los filtros de imagen son herramientas que modifican una imagen para lograr ciertos efectos, como resaltar bordes o suavizar los detalles.</p>
<p>Estos filtros son como pequeñas reglas matemáticas que aplicamos a cada punto de una imagen. En lugar de aplicarlos a mano, usamos una matriz de números (que llamamos núcleo de imagen) para hacer esto automáticamente.</p>
<p><strong>¿Cómo funcionan los filtros?</strong></p>
<p>Imagina que tienes una radiografía y quieres resaltar áreas importantes, como los bordes de los pulmones. Un filtro puede ayudarnos a hacer esto aplicando su matriz de números a cada parte de la imagen. Este proceso se llama convolución.</p>
<p>Cuando aplicamos un filtro a una imagen, básicamente:</p>
<ol type="1">
<li><strong>Multiplicamos</strong> los valores de los píxeles en la imagen por los valores del filtro (que llamamos <strong>pesos</strong>).</li>
<li>Luego <strong>sumamos</strong> esos valores multiplicados para obtener un nuevo valor para cada parte de la imagen.</li>
</ol>
<p>Este proceso se repite en toda la imagen, permitiendo que se transforme de manera útil. Por ejemplo, podemos desenfocar una radiografía o encontrar bordes importantes en una tomografía.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="resources/filters.png" class="img-fluid figure-img"></p>
<figcaption>filter</figcaption>
</figure>
</div>
<div id="a2542a39" class="cell" data-execution_count="1">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pydicom</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> cv2</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Cargar la imagen DICOM</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>dicom_file <span class="op">=</span> <span class="st">'resources/0000a175-0e68-4ca4-b1af-167204a7e0bc.dcm'</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>dicom_data <span class="op">=</span> pydicom.dcmread(dicom_file)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Obtener la imagen en formato Numpy</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>image <span class="op">=</span> dicom_data.pixel_array</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Aplicar el filtro Sobel para detectar bordes horizontales y verticales</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>sobelx <span class="op">=</span> cv2.Sobel(image, cv2.CV_64F, <span class="dv">1</span>, <span class="dv">0</span>, ksize<span class="op">=</span><span class="dv">5</span>)  <span class="co"># Sobel en dirección x</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>sobely <span class="op">=</span> cv2.Sobel(image, cv2.CV_64F, <span class="dv">0</span>, <span class="dv">1</span>, ksize<span class="op">=</span><span class="dv">5</span>)  <span class="co"># Sobel en dirección y</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Calcular la magnitud de los bordes combinados</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>sobel_combined <span class="op">=</span> np.sqrt(sobelx<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> sobely<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Normalizar la imagen filtrada al rango 0-255</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>sobel_combined <span class="op">=</span> cv2.normalize(sobel_combined, <span class="va">None</span>, <span class="dv">0</span>, <span class="dv">255</span>, cv2.NORM_MINMAX)</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Convertir a uint8 para visualización</span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>sobel_combined <span class="op">=</span> np.uint8(sobel_combined)</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Aplicar ecualización de histograma para mejorar el contraste</span></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>sobel_contrast <span class="op">=</span> cv2.equalizeHist(sobel_combined)</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Mostrar la imagen original y la imagen filtrada</span></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>))</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Imagen Original'</span>)</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>plt.imshow(image, cmap<span class="op">=</span><span class="st">'gray'</span>)</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Imagen Filtrada'</span>)</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>plt.imshow(sobel_contrast, cmap<span class="op">=</span><span class="st">'gray'</span>)</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Clasificador_files/figure-html/cell-2-output-1.png" width="815" height="400" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p><a href="setosa.io/ev/image-kernels">Ejemplo interactivo</a></p>
</section>
</section>
<section id="cnns" class="level2" data-number="4.3">
<h2 data-number="4.3" class="anchored" data-anchor-id="cnns"><span class="header-section-number">4.3</span> CNNs</h2>
<p><strong>¿Por qué no usamos siempre redes neuronales normales?</strong></p>
<p>Si usáramos redes neuronales normales para imágenes médicas, tendríamos que procesar una enorme cantidad de datos. Las imágenes tienen muchos píxeles, y para una red neuronal artificial normal, esto significaría cientos de miles o incluso millones de parámetros que necesitaríamos entrenar. Además, estas redes normales tienen un problema: pierden la estructura de la imagen cuando convertimos los datos.</p>
<p><strong>¿Cómo nos ayudan las redes neuronales convolucionales (CNN)?</strong></p>
<p>Las CNN están diseñadas para trabajar mejor con imágenes. Utilizan lo que se llama una capa de convolución, que es una forma especial de aplicar filtros de imagen. Estos filtros son como pequeñas “reglas” que recorren la imagen, detectando características importantes, como bordes o patrones.</p>
<p>En lugar de conectarse a todas las neuronas de la red como en las redes normales, las CNN solo conectan algunas neuronas de forma localizada, lo que permite reducir el número de datos a procesar y enfocarse en las áreas importantes.</p>
<p><strong>¿Cómo aprenden las CNN?</strong></p>
<p>Las CNN aprenden solas a descubrir los filtros más útiles. Por ejemplo, si estamos analizando imágenes de pulmón, una CNN podría aprender a detectar patrones que indican neumonía o tumores. Estos patrones pueden estar en cualquier parte de la imagen, y la CNN los encontrará aunque no estén en el centro de la imagen, algo que no podría hacer una red neuronal normal.</p>
<p>Además, las CNN son capaces de manejar imágenes en color, ya que dividen la imagen en diferentes canales (rojo, verde y azul) y aplican los filtros a cada canal. Esto permite que la red analice imágenes médicas de alta resolución y detalle.</p>
<p><strong>¿Por qué usamos capas de convolución?</strong></p>
<p>Las capas de convolución permiten a la red detectar patrones dentro de los patrones. A medida que aplicamos más y más filtros a la imagen, la red puede identificar detalles más complejos, como bordes, formas, y eventualmente patrones específicos, como anomalías en una radiografía o tomografía.</p>
<p><strong>¿Qué pasa en una red neuronal convolucional?</strong></p>
<p>En las redes neuronales convolucionales, no tenemos que decidir qué filtro usar. La red, a través de su entrenamiento, aprende automáticamente cuáles son los filtros más útiles. Por ejemplo, puede aprender qué filtros resaltan mejor las áreas con patrones anormales que podrían indicar neumonía en una radiografía de tórax.</p>
<p>Al entrenar la red con muchas imágenes, el sistema ajusta los pesos de los filtros para que sean buenos detectando lo que necesitamos, como lesiones pulmonares, tumores, etc.</p>
<section id="pooling" class="level3" data-number="4.3.1">
<h3 data-number="4.3.1" class="anchored" data-anchor-id="pooling"><span class="header-section-number">4.3.1</span> Pooling</h3>
<p><strong>¿Por qué usamos capas de pooling?</strong> Aunque las redes convolucionales ayudan a reducir la cantidad de información que se procesa al analizar imágenes, seguimos teniendo muchos datos cuando trabajamos con imágenes grandes o en color. Aquí es donde entran las capas de pooling. Estas capas nos permiten reducir el tamaño de los datos manteniendo la información más importante.</p>
<p><strong>¿Cómo funciona una capa de pooling?</strong> Una capa de pooling funciona como un filtro que selecciona solo los valores más importantes en una pequeña sección de la imagen. Por ejemplo, si tomamos un área de 4x4 píxeles en una imagen, en lugar de mantener los 16 píxeles, podemos aplicar max pooling, que selecciona el valor más alto de ese grupo, reduciendo el tamaño de la imagen y la cantidad de datos a manejar.</p>
<p>Aunque perdemos algo de detalle, la información principal de la imagen se conserva. Esto nos permite procesar más rápido las imágenes y reducir la carga de la red.</p>
<div id="53bfb6a7" class="cell" data-execution_count="2">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pydicom</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> cv2</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Cargar la imagen DICOM</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>dicom_file <span class="op">=</span> <span class="st">'resources/0000a175-0e68-4ca4-b1af-167204a7e0bc.dcm'</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>dicom_data <span class="op">=</span> pydicom.dcmread(dicom_file)</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Obtener la imagen en formato Numpy</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>image <span class="op">=</span> dicom_data.pixel_array</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Aplicar el filtro Sobel para detectar bordes horizontales y verticales</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>sobelx <span class="op">=</span> cv2.Sobel(image, cv2.CV_64F, <span class="dv">1</span>, <span class="dv">0</span>, ksize<span class="op">=</span><span class="dv">5</span>)  <span class="co"># Sobel en dirección x</span></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>sobely <span class="op">=</span> cv2.Sobel(image, cv2.CV_64F, <span class="dv">0</span>, <span class="dv">1</span>, ksize<span class="op">=</span><span class="dv">5</span>)  <span class="co"># Sobel en dirección y</span></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Calcular la magnitud combinada de Sobel</span></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>sobel_combined <span class="op">=</span> np.sqrt(sobelx<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> sobely<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Normalizar la imagen para mejorar la visualización (escalar al rango 0-255)</span></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>sobel_combined <span class="op">=</span> cv2.normalize(sobel_combined, <span class="va">None</span>, <span class="dv">0</span>, <span class="dv">255</span>, cv2.NORM_MINMAX)</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Convertir a uint8 para visualización</span></span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>sobel_combined <span class="op">=</span> np.uint8(sobel_combined)</span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Aplicar ecualización de histograma para mejorar el contraste</span></span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>sobel_contrast <span class="op">=</span> cv2.equalizeHist(sobel_combined)</span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Aplicar Max Pooling</span></span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> max_pooling(image, pool_size<span class="op">=</span><span class="dv">2</span>):</span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>    pooled_image <span class="op">=</span> cv2.resize(image, </span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>                              (image.shape[<span class="dv">1</span>] <span class="op">//</span> pool_size, image.shape[<span class="dv">0</span>] <span class="op">//</span> pool_size), </span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a>                              interpolation<span class="op">=</span>cv2.INTER_NEAREST)</span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> pooled_image</span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a>pooled_image <span class="op">=</span> max_pooling(sobel_contrast)</span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a><span class="co"># Mostrar imágenes</span></span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="dv">5</span>))</span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">1</span>)</span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Imagen Original'</span>)</span>
<span id="cb2-43"><a href="#cb2-43" aria-hidden="true" tabindex="-1"></a>plt.imshow(image, cmap<span class="op">=</span><span class="st">'gray'</span>)</span>
<span id="cb2-44"><a href="#cb2-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-45"><a href="#cb2-45" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">2</span>)</span>
<span id="cb2-46"><a href="#cb2-46" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Convolución (Filtro Sobel con Contraste)'</span>)</span>
<span id="cb2-47"><a href="#cb2-47" aria-hidden="true" tabindex="-1"></a>plt.imshow(sobel_contrast, cmap<span class="op">=</span><span class="st">'gray'</span>)</span>
<span id="cb2-48"><a href="#cb2-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-49"><a href="#cb2-49" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">3</span>)</span>
<span id="cb2-50"><a href="#cb2-50" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Max Pooling con Contraste Mejorado'</span>)</span>
<span id="cb2-51"><a href="#cb2-51" aria-hidden="true" tabindex="-1"></a>plt.imshow(pooled_image, cmap<span class="op">=</span><span class="st">'gray'</span>)</span>
<span id="cb2-52"><a href="#cb2-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-53"><a href="#cb2-53" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Clasificador_files/figure-html/cell-3-output-1.png" width="1184" height="390" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<iframe width="560" height="315" src="https://www.youtube.com/embed/VKnoyiNxflk?si=hK9eoiAQmdRWD-pJ" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="">
</iframe>
<p><a href="https://poloclub.github.io/cnn-explainer/">CNN visualizer</a></p>
<p>Entrenaremos un clasificador para predecir si una radiografía de un paciente muestra signos de neumonía o no, basado en el <a href="https://www.kaggle.com/c/rsna-pneumonia-detection-challenge">Desafío de Detección de Neumonía de la RSNA</a></p>
</section>
</section>
<section id="obtención-de-datos" class="level2" data-number="4.4">
<h2 data-number="4.4" class="anchored" data-anchor-id="obtención-de-datos"><span class="header-section-number">4.4</span> Obtención de Datos</h2>
<p>En el artículo de Wang et al. <span class="citation" data-cites="Wang2017">(<a href="references.html#ref-Wang2017" role="doc-biblioref">Wang et al. 2017</a>)</span>, se presenta la base de datos ChestX-ray8, que incluye benchmarks para la clasificación y localización de enfermedades torácicas comunes.</p>
<p>Primero, descargamos los datos de <a href="https://www.kaggle.com/c/rsna-pneumonia-detection-challenge/data">Kaggle</a></p>
<p>Fuente Original: https://nihcc.app.box.com/v/ChestXray-NIHCC</p>
<ol type="1">
<li><strong>Escala del Dataset:</strong></li>
</ol>
<ul>
<li>El <strong>ChestX-ray8</strong> es una base de datos masiva que contiene <strong>108,948 imágenes de rayos X</strong> en vista frontal de <strong>32,717 pacientes únicos</strong>. Estas imágenes fueron recolectadas de sistemas de archivado y comunicación de imágenes (PACS) de un hospital, y abarcan un período desde <strong>1992 hasta 2015</strong>.</li>
<li>Cada imagen está etiquetada con una o múltiples de <strong>ocho enfermedades comunes del tórax</strong> (Atelectasia, Cardiomegalia, Derrame, Infiltración, Masa, Nódulo, Neumonía y Neumotórax).</li>
</ul>
<ol start="2" type="1">
<li><strong>Etiquetado mediante Procesamiento de Lenguaje Natural (NLP):</strong></li>
</ol>
<ul>
<li>Las etiquetas de las enfermedades se extrajeron automáticamente de los informes radiológicos asociados a cada imagen usando técnicas de NLP. Esto permitió generar etiquetas débilmente supervisadas, es decir, etiquetas a nivel de imagen sin la necesidad de anotación manual exhaustiva, lo que sería impracticable a esta escala.</li>
<li>Herramientas de NLP como <strong>DNorm</strong> y <strong>MetaMap</strong> fueron usadas para identificar y normalizar los conceptos de enfermedades a partir de los informes. También se desarrollaron reglas personalizadas para manejar la <strong>negación</strong> e <strong>incertidumbre</strong> en las anotaciones.</li>
</ul>
<ol start="3" type="1">
<li><strong>Desafíos Técnicos:</strong></li>
</ol>
<ul>
<li><strong>Dimensiones de las Imágenes:</strong> Las radiografías de tórax suelen tener dimensiones grandes (2000x3000 píxeles), lo que presenta desafíos tanto para el almacenamiento como para el procesamiento eficiente de las imágenes.</li>
<li><strong>Variabilidad en las Etiquetas:</strong> Las enfermedades torácicas presentan una gran variabilidad en su apariencia y tamaño dentro de las imágenes, lo que dificulta la clasificación y localización precisas.</li>
</ul>
<ol start="4" type="1">
<li><strong>Procesamiento de Imágenes:</strong></li>
</ol>
<ul>
<li>Las imágenes fueron redimensionadas a 1024x1024 píxeles para facilitar el procesamiento computacional sin perder detalles significativos.</li>
<li>Se generaron <strong>cuadros delimitadores (B-Boxes)</strong> para un subconjunto de imágenes, permitiendo la evaluación de la localización de patologías, aunque el etiquetado denso y detallado a esta escala es inviable.</li>
</ul>
<section id="consideraciones" class="level3" data-number="4.4.1">
<h3 data-number="4.4.1" class="anchored" data-anchor-id="consideraciones"><span class="header-section-number">4.4.1</span> <strong>Consideraciones</strong></h3>
<ol type="1">
<li><strong>Gran Volumen de Datos:</strong></li>
</ol>
<ul>
<li>Recolectar y procesar más de 100,000 imágenes radiológicas requiere una infraestructura robusta de almacenamiento y un sistema eficiente de manejo de datos. Esto incluye tanto hardware (servidores, GPUs) como software especializado para manejar grandes volúmenes de datos.</li>
</ul>
<ol start="2" type="1">
<li><strong>Automatización de Etiquetado:</strong></li>
</ol>
<ul>
<li>El etiquetado manual de datos a esta escala es impracticable, por lo que es crucial implementar técnicas avanzadas de procesamiento de lenguaje natural para extraer información relevante de los informes radiológicos. Esto también implica un esfuerzo significativo en la validación de las etiquetas generadas automáticamente para asegurar su precisión.</li>
</ul>
<ol start="3" type="1">
<li><strong>Gestión de la Calidad de los Datos:</strong></li>
</ol>
<ul>
<li>Implementar procedimientos rigurosos para asegurar la calidad de las etiquetas es fundamental, especialmente en entornos clínicos donde la precisión es crítica. Esto incluye la validación cruzada con datos anotados manualmente y la eliminación de etiquetas ruidosas o incorrectas.</li>
</ul>
<ol start="4" type="1">
<li><strong>Adaptación de Modelos de Deep Learning:</strong></li>
</ol>
<ul>
<li>Dado que los modelos preentrenados en conjuntos de datos generales como ImageNet no son directamente aplicables al dominio médico, se requiere ajustar y entrenar modelos de deep learning específicos para el análisis de imágenes médicas, lo que demanda grandes recursos computacionales y expertos en el campo.</li>
</ul>
<ol start="5" type="1">
<li><strong>Infraestructura Computacional:</strong></li>
</ol>
<ul>
<li>Se necesita una infraestructura computacional poderosa para manejar el entrenamiento de modelos con imágenes de alta resolución, lo que incluye el uso de múltiples GPUs y técnicas como la reducción del tamaño de los lotes de imágenes para manejar las limitaciones de memoria.</li>
</ul>
<ol start="6" type="1">
<li><strong>Consideraciones Éticas y de Privacidad:</strong></li>
</ol>
<ul>
<li>El manejo de datos médicos sensibles requiere estrictas medidas de seguridad y anonimización para cumplir con regulaciones de privacidad como HIPAA o GDPR, lo que añade una capa adicional de complejidad al manejo del dataset.</li>
</ul>
</section>
</section>
<section id="preprocesamiento" class="level2" data-number="4.5">
<h2 data-number="4.5" class="anchored" data-anchor-id="preprocesamiento"><span class="header-section-number">4.5</span> Preprocesamiento</h2>
<p>Este notebook realiza varias tareas críticas de preprocesamiento, este se refiere a una serie de pasos realizados para transformar las imágenes de rayos X originales y las etiquetas asociadas a un formato que pueda ser utilizado de manera eficiente por un modelo de inteligencia artificial (IA) o aprendizaje profundo. Estos pasos son fundamentales porque los datos <em>crudos</em>, tal como están, no siempre son ideales para ser introducidos directamente en un modelo.</p>
<section id="exploración" class="level3" data-number="4.5.1">
<h3 data-number="4.5.1" class="anchored" data-anchor-id="exploración"><span class="header-section-number">4.5.1</span> Exploración</h3>
<div id="5142b1f9" class="cell" data-results="hide" data-execution_count="4">
<div class="cell-output cell-output-display" data-execution_count="4">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">patientId</th>
<th data-quarto-table-cell-role="th">x</th>
<th data-quarto-table-cell-role="th">y</th>
<th data-quarto-table-cell-role="th">width</th>
<th data-quarto-table-cell-role="th">height</th>
<th data-quarto-table-cell-role="th">Target</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>0004cfab-14fd-4e49-80ba-63a80b6bddd6</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>00313ee0-9eaa-42f4-b0ab-c148ed3241cd</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>00322d4d-1c29-4943-afc9-b6754be640eb</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>003d8fa0-6bf1-40ed-b54c-ac657f8495c5</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>00436515-870c-4b36-a041-de91049b9ab4</td>
<td>264.0</td>
<td>152.0</td>
<td>213.0</td>
<td>379.0</td>
<td>1</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">5</td>
<td>00436515-870c-4b36-a041-de91049b9ab4</td>
<td>562.0</td>
<td>152.0</td>
<td>256.0</td>
<td>453.0</td>
<td>1</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>Leemos el archivo CSV que contiene las etiquetas asociadas a las imágenes. Cada fila contiene un patientId, coordenadas para posibles consolidaciomes (si se detecta neumonía), y la variable <code>Target</code>, que indica si la imagen tiene o no signos de neumonía.</p>
<p>El Target es binario (1 = neumonía, 0 = no neumonía). Esta es la variable objetivo que el modelo aprenderá a predecir.</p>
<div id="71e8cd3c" class="cell" data-results="hide" data-execution_count="5">
<div class="cell-output cell-output-display" data-execution_count="5">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">patientId</th>
<th data-quarto-table-cell-role="th">x</th>
<th data-quarto-table-cell-role="th">y</th>
<th data-quarto-table-cell-role="th">width</th>
<th data-quarto-table-cell-role="th">height</th>
<th data-quarto-table-cell-role="th">Target</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>0004cfab-14fd-4e49-80ba-63a80b6bddd6</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>00313ee0-9eaa-42f4-b0ab-c148ed3241cd</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>00322d4d-1c29-4943-afc9-b6754be640eb</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>003d8fa0-6bf1-40ed-b54c-ac657f8495c5</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>00436515-870c-4b36-a041-de91049b9ab4</td>
<td>264.0</td>
<td>152.0</td>
<td>213.0</td>
<td>379.0</td>
<td>1</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">6</td>
<td>00569f44-917d-4c86-a842-81832af98c30</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>0</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>Se eliminan duplicados en las filas que contienen el mismo <code>patientId</code>. Esto es importante porque tener múltiples entradas para el mismo paciente podría causar problemas en el entrenamiento del modelo, como sesgo o sobreajuste.</p>
<p>Cada paciente debe ser representado una única vez en el análisis, para evitar una ponderación excesiva de imágenes de un mismo paciente.</p>
<div id="75449a8b" class="cell" data-execution_count="6">
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Clasificador_files/figure-html/cell-7-output-1.png" width="592" height="446" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Visualizamos la distribución de las etiquetas para identificar cualquier desbalance en los datos. Un fuerte desbalance, como un número desproporcionado de imágenes sin neumonía, puede afectar el desempeño del modelo y requerir estrategias como submuestreo o sobrepeso en la clase minoritaria.</p>
<p>Este gráfico ayuda a entender la prevalencia de neumonía en el conjunto de datos. Si el número de casos de neumonía es mucho menor, eso refleja un reto diagnóstico similar al del mundo real.</p>
<div id="738b913d" class="cell" data-execution_count="8">
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Clasificador_files/figure-html/cell-9-output-1.png" width="743" height="727" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Creamos una cuadrícula 3 x 3 para visualizar 9 imágenes. Esta técnica de visualización ayuda a explorar los datos visualmente, verificando si la calidad de las imágenes es adecuada para el entrenamiento de un modelo.</p>
</section>
<section id="preprocesamiento-de-imagenes" class="level3" data-number="4.5.2">
<h3 data-number="4.5.2" class="anchored" data-anchor-id="preprocesamiento-de-imagenes"><span class="header-section-number">4.5.2</span> Preprocesamiento de Imagenes</h3>
<div id="2ed17a0b" class="cell" data-execution_count="9">
<div class="cell-output cell-output-stdout">
<pre><code>Metadata del archivo DICOM:
Dataset.file_meta -------------------------------
(0002, 0000) File Meta Information Group Length  UL: 202
(0002, 0001) File Meta Information Version       OB: b'\x00\x01'
(0002, 0002) Media Storage SOP Class UID         UI: Secondary Capture Image Storage
(0002, 0003) Media Storage SOP Instance UID      UI: 1.2.276.0.7230010.3.1.4.8323329.10118.1517874346.924223
(0002, 0010) Transfer Syntax UID                 UI: JPEG Baseline (Process 1)
(0002, 0012) Implementation Class UID            UI: 1.2.276.0.7230010.3.0.3.6.0
(0002, 0013) Implementation Version Name         SH: 'OFFIS_DCMTK_360'
-------------------------------------------------
(0008, 0005) Specific Character Set              CS: 'ISO_IR 100'
(0008, 0016) SOP Class UID                       UI: Secondary Capture Image Storage
(0008, 0018) SOP Instance UID                    UI: 1.2.276.0.7230010.3.1.4.8323329.10118.1517874346.924223
(0008, 0020) Study Date                          DA: '19010101'
(0008, 0030) Study Time                          TM: '000000.00'
(0008, 0050) Accession Number                    SH: ''
(0008, 0060) Modality                            CS: 'CR'
(0008, 0064) Conversion Type                     CS: 'WSD'
(0008, 0090) Referring Physician's Name          PN: ''
(0008, 103e) Series Description                  LO: 'view: PA'
(0010, 0010) Patient's Name                      PN: '0000a175-0e68-4ca4-b1af-167204a7e0bc'
(0010, 0020) Patient ID                          LO: '0000a175-0e68-4ca4-b1af-167204a7e0bc'
(0010, 0030) Patient's Birth Date                DA: ''
(0010, 0040) Patient's Sex                       CS: 'F'
(0010, 1010) Patient's Age                       AS: '46'
(0018, 0015) Body Part Examined                  CS: 'CHEST'
(0018, 5101) View Position                       CS: 'PA'
(0020, 000d) Study Instance UID                  UI: 1.2.276.0.7230010.3.1.2.8323329.10118.1517874346.924222
(0020, 000e) Series Instance UID                 UI: 1.2.276.0.7230010.3.1.3.8323329.10118.1517874346.924221
(0020, 0010) Study ID                            SH: ''
(0020, 0011) Series Number                       IS: '1'
(0020, 0013) Instance Number                     IS: '1'
(0020, 0020) Patient Orientation                 CS: ''
(0028, 0002) Samples per Pixel                   US: 1
(0028, 0004) Photometric Interpretation          CS: 'MONOCHROME2'
(0028, 0010) Rows                                US: 1024
(0028, 0011) Columns                             US: 1024
(0028, 0030) Pixel Spacing                       DS: [0.19431099999999998, 0.19431099999999998]
(0028, 0100) Bits Allocated                      US: 8
(0028, 0101) Bits Stored                         US: 8
(0028, 0102) High Bit                            US: 7
(0028, 0103) Pixel Representation                US: 0
(0028, 2110) Lossy Image Compression             CS: '01'
(0028, 2114) Lossy Image Compression Method      CS: 'ISO_10918_1'
(7fe0, 0010) Pixel Data                          OB: Array of 124058 elements

Características de información del archivo DICOM:
Paciente: 0000a175-0e68-4ca4-b1af-167204a7e0bc
ID del Paciente: 0000a175-0e68-4ca4-b1af-167204a7e0bc
Modalidad: CR
Fecha del estudio: 19010101
Tamaño de la imagen: 1024 x 1024
Número de fotogramas: 1
Profundidad de bits: 8 bits</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Clasificador_files/figure-html/cell-10-output-2.png" width="389" height="409" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Para manejar eficientemente nuestros datos , convertimos las imágenes de rayos X almacenadas en formato DICOM a matrices.</p>
<p>Posteriormente, calculamos la media y la desviación estándar general de los píxeles de todo el conjunto de datos con el propósito de normalización.</p>
<p>Luego, las imágenes en matrices creadas se almacenan en dos carpetas separadas según su etiqueta binaria: * 0: Todas las radiografías que no muestran signos de neumonía * 1: Todas las radiografías que muestran signos de neumonía</p>
<p>Estandarizamos todas las imágenes utilizando el valor máximo de píxel en el conjunto de datos proporcionado, 255. Todas las imágenes se redimensionan a 224x224.</p>
<div id="d959ecbf" class="cell" data-execution_count="10">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>sums <span class="op">=</span> <span class="dv">0</span>  <span class="co"># Inicializa la variable para acumular la suma de los píxeles</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>sums_squared <span class="op">=</span> <span class="dv">0</span>  <span class="co"># Inicializa la variable para acumular la suma de los cuadrados de los píxeles</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Itera sobre el DataFrame de etiquetas, obteniendo el índice (c) y el ID del paciente (patient_id)</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> c, patient_id <span class="kw">in</span> <span class="bu">enumerate</span>(tqdm(labels.patientId)):  </span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Crea la ruta completa al archivo DICOM correspondiente al paciente</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    dcm_path <span class="op">=</span> ROOT_PATH<span class="op">/</span>patient_id  </span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>    dcm_path <span class="op">=</span> dcm_path.with_suffix(<span class="st">".dcm"</span>)  <span class="co"># Añade la extensión ".dcm" al archivo para que sea legible como DICOM</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Lee el archivo DICOM usando pydicom y normaliza los valores de los píxeles dividiendo entre 255</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>    dcm <span class="op">=</span> pydicom.read_file(dcm_path).pixel_array <span class="op">/</span> <span class="dv">255</span>  </span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Redimensiona la imagen, ya que 1024x1024 es demasiado grande para manejar en modelos de Deep Learning.</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Cambiamos a una resolución de 224x224.</span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Convertimos la imagen a tipo float16 para usar menos memoria al almacenar la imagen.</span></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>    dcm_array <span class="op">=</span> cv2.resize(dcm, (<span class="dv">224</span>, <span class="dv">224</span>)).astype(np.float16)</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Recupera la etiqueta correspondiente a la imagen del paciente (0 para sano, 1 para neumonía)</span></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>    label <span class="op">=</span> labels.Target.iloc[c]</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Divide el conjunto de datos en 4/5 para entrenamiento y 1/5 para validación</span></span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>    train_or_val <span class="op">=</span> <span class="st">"train"</span> <span class="cf">if</span> c <span class="op">&lt;</span> <span class="dv">24000</span> <span class="cf">else</span> <span class="st">"val"</span>  </span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Define la ruta de guardado y crea las carpetas necesarias si no existen</span></span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>    current_save_path <span class="op">=</span> SAVE_PATH<span class="op">/</span>train_or_val<span class="op">/</span><span class="bu">str</span>(label)  </span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>    current_save_path.mkdir(parents<span class="op">=</span><span class="va">True</span>, exist_ok<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Guarda el array de la imagen en el directorio correspondiente (train/val y clase 0 o 1)</span></span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>    np.save(current_save_path<span class="op">/</span>patient_id, dcm_array)  </span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Normaliza la suma de los píxeles dividiendo por el número total de píxeles en la imagen (224x224)</span></span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a>    normalizer <span class="op">=</span> dcm_array.shape[<span class="dv">0</span>] <span class="op">*</span> dcm_array.shape[<span class="dv">1</span>]  </span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Solo calcula estadísticas de las imágenes de entrenamiento (no para validación)</span></span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> train_or_val <span class="op">==</span> <span class="st">"train"</span>:  </span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Suma los valores de los píxeles normalizados de cada imagen para calcular la media posteriormente</span></span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a>        sums <span class="op">+=</span> np.<span class="bu">sum</span>(dcm_array) <span class="op">/</span> normalizer  </span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Suma los cuadrados de los píxeles normalizados de cada imagen para calcular la desviación estándar posteriormente</span></span>
<span id="cb4-40"><a href="#cb4-40" aria-hidden="true" tabindex="-1"></a>        sums_squared <span class="op">+=</span> (np.power(dcm_array, <span class="dv">2</span>).<span class="bu">sum</span>()) <span class="op">/</span> normalizer  </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"435a6052653e46b6915ec7196ab878ff","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
</div>
<section id="calcular-media-y-desviación-estándar-del-dataset" class="level4" data-number="4.5.2.1">
<h4 data-number="4.5.2.1" class="anchored" data-anchor-id="calcular-media-y-desviación-estándar-del-dataset"><span class="header-section-number">4.5.2.1</span> Calcular Media y Desviación Estándar del Dataset</h4>
<p>Para calcular la media y la desviación estándar del conjunto de datos, calculamos la suma de los valores de los píxeles, así como la suma de los valores de píxeles al cuadrado para cada sujeto. Esto permite calcular la media y la desviación estándar general sin mantener todo el conjunto de datos en memoria.</p>
<div id="43c4e0cc" class="cell" data-execution_count="11">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>mean <span class="op">=</span> sums <span class="op">/</span> <span class="dv">24000</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>std <span class="op">=</span> np.sqrt(sums_squared <span class="op">/</span> <span class="dv">24000</span> <span class="op">-</span> (mean<span class="op">**</span><span class="dv">2</span>))</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Mean of Dataset: </span><span class="sc">{</span>mean<span class="sc">}</span><span class="ss">, STD: </span><span class="sc">{</span>std<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Mean of Dataset: 0.49039623525191567, STD: 0.2479507326197431</code></pre>
</div>
</div>
<p><strong>Media</strong></p>
<p><span class="math inline">\(\mu = \frac{\text{sums}}{24000}\)</span></p>
<p>Donde <code>sums</code> es la suma acumulada de los valores de píxeles de todas las imágenes de entrenamiento, y <code>24000</code> es el número total de imágenes en el conjunto de entrenamiento.</p>
<p><strong>Desviación Estándar</strong></p>
<p><span class="math display">\[\sigma = \sqrt{\frac{\text{sums_squared}}{24000} - \mu^2}\]</span></p>
<p>Donde: - <code>sums_squared</code> es la suma acumulada de los cuadrados de los valores de los píxeles. - <code>mean**2</code> es el cuadrado de la media que ya se ha calculado.</p>
<p>La normalización es importante para asegurarse de que los valores de los píxeles estén en un rango que permita a las redes neuronales converger más rápido y con mayor precisión. Al normalizar, centramos los valores en torno a la media (0.49) y escalamos con la desviación estándar.</p>
<p>Este paso asegura que las variaciones en brillo o contraste no afecten el rendimiento del modelo de manera injustificada, permitiendo que la red neuronal se concentre en las características importantes para el diagnóstico, como las consolidaciones pulmonares.</p>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-Wang2017" class="csl-entry" role="listitem">
Wang, Xiaosong, Yifan Peng, Le Lu, Zhiyong Lu, Mahyar Bagheri, and Ronald M. Summers. 2017. <span>“ChestX-Ray8: Hospital-Scale Chest x-Ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases.”</span> In <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>. <a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Wang_ChestX-ray8_Hospital-Scale_Chest_CVPR_2017_paper.pdf">http://openaccess.thecvf.com/content_cvpr_2017/papers/Wang_ChestX-ray8_Hospital-Scale_Chest_CVPR_2017_paper.pdf</a>.
</div>
</div>
</section>
</section>
</section>

</main> <!-- /main -->
<script type="application/vnd.jupyter.widget-state+json">
{"state":{"2357b51b733a4f1ca79b74a52872f81f":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_4addfdab110e4f4a83740faaad8ef7ab","placeholder":"​","style":"IPY_MODEL_3a1c6b5881de4ef9a9d7081d2f43f937","tabbable":null,"tooltip":null,"value":"100%"}},"250d67226b1c45008144dc19563017ea":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_7bb14dcaa70b4bd089741c7354910b43","placeholder":"​","style":"IPY_MODEL_87c16a26e09741fe9510857e735a7128","tabbable":null,"tooltip":null,"value":" 26684/26684 [02:50&lt;00:00, 176.85it/s]"}},"3a1c6b5881de4ef9a9d7081d2f43f937":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"435a6052653e46b6915ec7196ab878ff":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2357b51b733a4f1ca79b74a52872f81f","IPY_MODEL_af9d73cace1142ad9b8f298127384886","IPY_MODEL_250d67226b1c45008144dc19563017ea"],"layout":"IPY_MODEL_63dd19c96114483e8d5fea9bd1cd6bf8","tabbable":null,"tooltip":null}},"4addfdab110e4f4a83740faaad8ef7ab":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"586a1426782840c7ba4169a98d10d708":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"63dd19c96114483e8d5fea9bd1cd6bf8":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7bb14dcaa70b4bd089741c7354910b43":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"87c16a26e09741fe9510857e735a7128":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"abe1a76b114c4045bee2f9b3fd4c391c":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"af9d73cace1142ad9b8f298127384886":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_abe1a76b114c4045bee2f9b3fd4c391c","max":26684,"min":0,"orientation":"horizontal","style":"IPY_MODEL_586a1426782840c7ba4169a98d10d708","tabbable":null,"tooltip":null,"value":26684}}},"version_major":2,"version_minor":0}
</script>
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./colaboracion.html" class="pagination-link" aria-label="Colaboración Interdisciplinaria en Proyectos de IA">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Colaboración Interdisciplinaria en Proyectos de IA</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./Entrenamiento.html" class="pagination-link" aria-label="Entrenamiento">
        <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Entrenamiento</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>