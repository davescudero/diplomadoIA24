# Entrenamiento

En este notebook, crearemos y entrenaremos un clasificador en el conjunto de datos de rayos X de t칩rax para clasificar si una imagen muestra signos de neumon칤a o no.

```{python}
#| echo: false
#| results: hide
import torch
import torchvision
from torchvision import transforms
import torchmetrics
import pytorch_lightning as pl
from pytorch_lightning.callbacks import ModelCheckpoint
from pytorch_lightning.loggers import TensorBoardLogger
from tqdm.notebook import tqdm
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
```

## Transformaciones para Aumentaci칩n y Normalizaci칩n de Datos

- Transformaciones en entrenamiento: Se usan para aumentar los datos y evitar sobreajuste. Incluye recortes, rotaciones, traslaciones, y cambios de escala para que el modelo aprenda a generalizar mejor.

- Transformaciones en validaci칩n: Solo incluye la normalizaci칩n, ya que no queremos alterar las im치genes durante la evaluaci칩n.


Usamos:
- un recorte aleatorio de la imagen y la redimensiona al tama침o original (224x224).
- Rotaciones aleatorias entre -5 y 5 grados.
- Traslaci칩n aleatoria (m치ximo 5%).
- Escalado aleatorio (0.9-1.1 del tama침o original de la imagen).

```{python}
#| code-fold: true
train_transforms = transforms.Compose([
                                    transforms.ToTensor(),  # Convert numpy array to tensor
                                    transforms.Normalize(0.49, 0.248),  # Use mean and std from preprocessing notebook
                                    transforms.RandomAffine( # Data Augmentation
                                        degrees=(-5, 5), translate=(0, 0.05), scale=(0.9, 1.1)),
                                        transforms.RandomResizedCrop((224, 224), scale=(0.35, 1))

])

val_transforms = transforms.Compose([
                                    transforms.ToTensor(),  # Convert numpy array to tensor
                                    transforms.Normalize([0.49], [0.248]),  # Use mean and std from preprocessing notebook
])


```

```{python}
#| echo: false
#| results: hide
from utils import load_file

train_dataset = torchvision.datasets.DatasetFolder(
    "/Users/davidescudero/Library/Mobile Documents/com~apple~CloudDocs/Documents/Github/diplomadoIA24/Processed/train",
    loader=load_file, extensions="npy", transform=train_transforms)

val_dataset = torchvision.datasets.DatasetFolder(
    "/Users/davidescudero/Library/Mobile Documents/com~apple~CloudDocs/Documents/Github/diplomadoIA24/Processed/val",
    loader=load_file, extensions="npy", transform=val_transforms)
```

```{python}
#| echo: false
fig, axis = plt.subplots(2, 2, figsize=(9, 9))
for i in range(2):
    for j in range(2):
        random_index = np.random.randint(0, 20000)
        x_ray, label = train_dataset[random_index]
        axis[i][j].imshow(x_ray[0], cmap="bone")
        axis[i][j].set_title(f"Label:{label}")
```

```{python}
#| code-fold: true
batch_size = 64
num_workers = 4

train_loader = torch.utils.data.DataLoader(
    train_dataset, 
    batch_size=batch_size, 
    num_workers=num_workers, 
    shuffle=True, 
    persistent_workers=True 
)

val_loader = torch.utils.data.DataLoader(
    val_dataset, 
    batch_size=batch_size, 
    num_workers=num_workers, 
    shuffle=False, 
    persistent_workers=True 
)

print(f"Tenemos {len(train_dataset)} imagenes de entrenamiento y  {len(val_dataset)} imagenes de validaci칩n")
```


```{python}
#| echo: false
#| results: hide
np.unique(train_dataset.targets, return_counts=True), np.unique(val_dataset.targets, return_counts=True)
```

## Manejo del Desbalanceo de clases

Las clases est치n desbalanceadas: hay m치s im치genes sin signos de neumon칤a que con neumon칤a. Existen varias formas de manejar conjuntos de datos desbalanceados:

- P칠rdida ponderada (Weighted Loss)
- Sobremuestreo (Oversampling)
- No hacer nada 游뗵

## Creacion del Modelo

```python
class PneumoniaModel(pl.LightningModule):
    def __init__(self, weight=1):
        super().__init__()
        
        self.model = torchvision.models.resnet18()
        self.model.conv1 = torch.nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
        self.model.fc = torch.nn.Linear(in_features=512, out_features=1)
        
        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-4)
        self.loss_fn = torch.nn.BCEWithLogitsLoss(pos_weight=torch.tensor([weight]))
        
        self.train_acc = torchmetrics.Accuracy(task="binary")
        self.val_acc = torchmetrics.Accuracy(task="binary")
```

- ResNet-18: Se modifica para aceptar im치genes en escala de grises (un solo canal, en lugar de tres como en im치genes RGB).
- Binary Cross-Entropy with Logits: Esta funci칩n combina la sigmoide y la entrop칤a cruzada en una sola operaci칩n, y aqu칤 se ajusta para darle m치s peso a la clase minoritaria (neumon칤a).
- Accuracies: Se usan para rastrear la precisi칩n en cada paso de entrenamiento y validaci칩n.

## Entrenamiento y validaci칩n

El flujo para el entrenamiento y validaci칩n es muy similar:

- En `training_step`, se calcula la p칠rdida y la precisi칩n, y se almacenan para usarlas al final de la 칠poca.
- En `validation_step`, se hace lo mismo pero en el conjunto de validaci칩n, sin actualizar los pesos del modelo.

## Optimizador y Devoluci칩n de Llamadas

- Se usa un solo GPU para el entrenamiento. Si hay m치s GPUs disponibles, se puede ajustar el n칰mero.
- `ModelCheckpoint`: Guarda el mejor modelo basado en la precisi칩n de validaci칩n.

[Trainer documentation](https://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html)

## Evaluacion Final

Se miden m칠tricas como la precisi칩n (accuracy), recall, y la matriz de confusi칩n, que se grafican para analizar el rendimiento del modelo.

```{python}
#| echo: false
import pickle
import matplotlib.pyplot as plt
import seaborn as sns
# Cargar las m칠tricas desde el archivo pickle
with open("metrics.pkl", "rb") as f:
    metrics = pickle.load(f)

# Mostrar las m칠tricas
print(f"Accuracy: {metrics['accuracy']}")
print(f"Precision: {metrics['precision']}")
print(f"Recall: {metrics['recall']}")

# Visualizar la matriz de confusi칩n
fig, ax = plt.subplots(1, 2, figsize=(12, 5))

sns.heatmap(metrics['confusion_matrix'], annot=True, fmt="d", cmap="Blues", ax=ax[0])
ax[0].set_title("Confusion Matrix")

sns.heatmap(metrics['confusion_matrix_threshold'], annot=True, fmt="d", cmap="Blues", ax=ax[1])
ax[1].set_title("Confusion Matrix with Threshold=0.25")

plt.show()
```


Las m칠tricas clave en este tipo de problemas de clasificaci칩n binaria (como la detecci칩n de neumon칤a a partir de im치genes de rayos X) son **precisi칩n**, **recall (sensibilidad)**, **matriz de confusi칩n**, y **exactitud (accuracy)**.

### **Exactitud (Accuracy)**

La exactitud es la proporci칩n de predicciones correctas (tanto verdaderos positivos como verdaderos negativos) con respecto al total de predicciones realizadas.


$\text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}$


- **TP**: Verdaderos positivos (correctamente clasificados como con neumon칤a).
- **TN**: Verdaderos negativos (correctamente clasificados como sin neumon칤a).
- **FP**: Falsos positivos (incorrectamente clasificados como con neumon칤a).
- **FN**: Falsos negativos (incorrectamente clasificados como sin neumon칤a).

En el caso de nuestro modelo, por ejemplo, un valor de **0.8457** de exactitud indica que el 84.57% de las predicciones fueron correctas, es decir, el modelo clasific칩 correctamente tanto las im치genes que mostraban neumon칤a como las que no.

**Limitaci칩n**: En datasets desbalanceados (como en este caso), la exactitud puede ser enga침osa, ya que un modelo puede obtener alta exactitud solo por predecir siempre la clase mayoritaria (en este caso, probablemente las radiograf칤as sin neumon칤a). Por eso, necesitamos otras m칠tricas como **precision** y **recall**.

### **Precisi칩n (Precision)**

**Definici칩n**: La precisi칩n mide qu칠 proporci칩n de las predicciones positivas (el modelo dice que hay neumon칤a) son realmente correctas.


$\text{Precision} = \frac{TP}{TP + FP}$

La precisi칩n de **0.7002** significa que, de todas las veces que el modelo predijo neumon칤a (incluyendo falsos positivos), el 70.02% de esas predicciones fueron correctas.


### **Recall**

El **recall** o **sensibilidad** mide qu칠 proporci칩n de los casos positivos reales (personas con neumon칤a) fueron correctamente identificados por el modelo.


$\text{Recall} = \frac{TP}{TP + FN}$


En nuestro modelo, el recall es **0.5520**, lo que significa que el modelo identific칩 correctamente el 55.20% de los pacientes con neumon칤a.

Un bajo valor de **recall** indica que el modelo est치 fallando en detectar algunos casos de neumon칤a (falsos negativos). Esto es problem치tico en contextos cl칤nicos porque significa que algunas personas que realmente tienen neumon칤a no ser치n diagnosticadas por el modelo, lo cual es cr칤tico en t칠rminos de tratamiento.

### **Matriz de Confusi칩n**

La matriz de confusi칩n ofrece una representaci칩n m치s detallada de las predicciones del modelo. Se desglosa en cuatro valores clave:

- **TP (Verdaderos Positivos)**: Casos de neumon칤a correctamente clasificados.
- **TN (Verdaderos Negativos)**: Casos sin neumon칤a correctamente clasificados.
- **FP (Falsos Positivos)**: Casos sin neumon칤a que fueron clasificados como neumon칤a.
- **FN (Falsos Negativos)**: Casos de neumon칤a que fueron clasificados como sanos.

```
Confusion Matrix:
tensor([[1936,  143],
        [ 271,  334]])
```


- **1936**: Verdaderos negativos (el modelo predijo correctamente que no hab칤a neumon칤a).
- **143**: Falsos positivos (el modelo predijo neumon칤a cuando no la hab칤a).
- **271**: Falsos negativos (el modelo no detect칩 neumon칤a cuando s칤 la hab칤a).
- **334**: Verdaderos positivos (el modelo detect칩 correctamente los casos de neumon칤a).

Una matriz de confusi칩n como esta te permite ver en qu칠 est치 fallando el modelo, en este caso, hay m치s falsos negativos que falsos positivos, lo que es una preocupaci칩n en aplicaciones cl칤nicas.

### 5. **Matriz de Confusi칩n con Umbral Ajustado**

```
Confusion Matrix (Threshold=0.25):
tensor([[1725,  354],
        [ 136,  469]])
```

Al cambiar el umbral de decisi칩n a **0.25**, el modelo se vuelve m치s propenso a clasificar como neumon칤a, lo que reduce los **falsos negativos** (de 271 a 136), pero incrementa los **falsos positivos** (de 143 a 354).

Este ajuste en el umbral es 칰til si prefieres minimizar los falsos negativos (casos de neumon칤a no detectados) a costa de aumentar algunos falsos positivos (personas sanas que son clasificadas incorrectamente como con neumon칤a).

### **Trade-offs Entre Precisi칩n y Recall**:

- **Alta precisi칩n y bajo recall**: El modelo es muy preciso en sus predicciones, pero podr칤a estar perdiendo muchos casos positivos (en este caso, personas con neumon칤a).
- **Alta recall y baja precisi칩n**: El modelo detecta la mayor칤a de los casos positivos, pero tambi칠n produce m치s falsos positivos.

### **Consideraciones Cl칤nicas**:
- En un contexto m칠dico, es crucial mantener un equilibrio entre **recall** y **precisi칩n**, dependiendo del impacto de los falsos positivos y falsos negativos.
- En el caso de neumon칤a, podr칤as priorizar un **alto recall** para no perder casos verdaderos, ya que no detectar neumon칤a a tiempo puede ser cr칤tico para la salud del paciente.

## Visualizacion de Predicciones

![Predicciones](resources/predictions.png)

